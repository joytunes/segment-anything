{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331a21f-fa89-427b-a581-a4952d102d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import coremltools as ct\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from segment_anything import sam_model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/Users/anatoli/Documents/segment-anything/checkpoints'\n",
    "\n",
    "model_type = 'vit_b' # 'vit_b', 'vit_l' or 'vit_h'\n",
    "if model_type == 'vit_h':\n",
    "    checkpoint = 'sam_vit_h_4b8939.pth'\n",
    "elif model_type == 'vit_b':\n",
    "    checkpoint = 'sam_vit_b_01ec64.pth'\n",
    "elif model_type == 'vit_l':\n",
    "    checkpoint = 'sam_vit_l_0b3195.pth'\n",
    "else:\n",
    "    raise ValueError(f'Unknown model type {model_type}, model type must be one of \"vit_b\", \"vit_l\", \"vit_h\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da638ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.utils.coreml import SamEmbedder, SamPointDecoder\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=os.path.join(checkpoint_dir, checkpoint))\n",
    "point_decoder_model = SamPointDecoder(sam, return_single_mask=False)\n",
    "image_embedder_model = SamEmbedder(sam)\n",
    "\n",
    "embed_dim = sam.prompt_encoder.embed_dim\n",
    "embed_size = sam.prompt_encoder.image_embedding_size\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "point_decoder_dummy_inputs = {\n",
    "    \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
    "    \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n",
    "    \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n",
    "    \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
    "    \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
    "}\n",
    "image_embedder_dummy_inputs = {\n",
    "    # The image as a torch tensor in 3xHxW format, already transformed for input to the model.\n",
    "    'image': torch.randint(low=0, high=255, size=(1, 3, 1024, 1024), dtype=torch.float),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "point_decoder_trace = torch.jit.trace(point_decoder_model.eval(), tuple(point_decoder_dummy_inputs.values()))\n",
    "image_embedder_trace = torch.jit.trace(image_embedder_model.eval(), tuple(image_embedder_dummy_inputs.values()))\n",
    "\n",
    "# Save the traced model\n",
    "torch.jit.save(point_decoder_trace, f'point_decoder_{model_type}_model.pt')\n",
    "torch.jit.save(image_embedder_trace, f'image_embedder_{model_type}_model.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759bfb6-84d5-4e2f-bf5c-fbd8bb5fe4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_decoder_coreml_inputs = {\n",
    "    \"image_embeddings\": ct.TensorType(name=\"image_embeddings\", shape=point_decoder_dummy_inputs[\"image_embeddings\"].size()),\n",
    "    \"point_coords\": ct.TensorType(name=\"point_coords\", shape=point_decoder_dummy_inputs[\"point_coords\"].size()),\n",
    "    \"point_labels\": ct.TensorType(name=\"point_labels\", shape=point_decoder_dummy_inputs[\"point_labels\"].size()),\n",
    "    \"mask_input\": ct.TensorType(name=\"mask_input\", shape=point_decoder_dummy_inputs[\"mask_input\"].size()),\n",
    "    \"has_mask_input\": ct.TensorType(name=\"has_mask_input\", shape=point_decoder_dummy_inputs[\"has_mask_input\"].size()),\n",
    "}\n",
    "point_decoder_coreml_outputs = {\n",
    "    \"iou_predictions\": ct.TensorType(name=\"iou_predictions\"),\n",
    "    \"low_res_masks\": ct.TensorType(name=\"low_res_masks\")\n",
    "}\n",
    "point_decoder_coreml_model = ct.convert(\n",
    "    point_decoder_trace,\n",
    "    outputs=list(point_decoder_coreml_outputs.values()),\n",
    "    inputs=list(point_decoder_coreml_inputs.values()),\n",
    "    minimum_deployment_target=ct.target.iOS15\n",
    ")\n",
    "\n",
    "image_embedder_coreml_inputs = {\n",
    "    'image': ct.ImageType(name='image', shape=image_embedder_dummy_inputs['image'].shape, channel_first=True),\n",
    "}\n",
    "image_embedder_coreml_outputs = {\n",
    "    \"image_embeddings\": ct.TensorType(name=\"image_embeddings\")\n",
    "}\n",
    "image_embedder_coreml_model = ct.convert(\n",
    "    image_embedder_trace,\n",
    "    outputs=list(image_embedder_coreml_outputs.values()),\n",
    "    inputs=list(image_embedder_coreml_inputs.values()),\n",
    "    minimum_deployment_target=ct.target.iOS15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e14642",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "point_decoder_coreml_model.save(f\"point_decoder_{model_type}.mlpackage\")\n",
    "image_embedder_coreml_model.save(f\"image_embedder_{model_type}.mlpackage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e7581",
   "metadata": {},
   "source": [
    "Everything below this point is for exporting the entire pytorch model (embeddings and all) directly using coreml, and not the embeddings-to-masks part via the onnx model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a34c0",
   "metadata": {},
   "source": [
    "The following cells allow you to test the model on a single image, without specifying prompts (points, bboxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed541c8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323943f8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "image = cv2.imread('/Users/anatoli/Downloads/Untitled_anatoli.jpeg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a6d08",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc2b02",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i, mask in enumerate(masks):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    show_mask(mask['segmentation'], plt.gca())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243dfa8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
