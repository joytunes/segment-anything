{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e874b-0706-4f90-9fac-54d799e29920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torchvision\n",
    "import coremltools as ct\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/Users/anatoli/Documents/segment-anything')\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import QuantType\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "checkpoint_dir = '/Users/anatoli/Documents/segment-anything/checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11a84e-8c1c-4d8e-b14e-f92e7ec6d02a",
   "metadata": {},
   "source": [
    "The script `segment-anything/scripts/export_onnx_model.py` can be used to export the necessary portion of SAM. Alternatively, run the following code to export an ONNX model. If you have already exported a model, set the path below and skip to the next section. Assure that the exported ONNX model aligns with the checkpoint and model type set above. This notebook expects the model was exported with the parameter `return_single_mask=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(MyModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *x):\n",
    "        output = self.model(*x)\n",
    "        # Modify output here\n",
    "        return (output[1], output[2])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da638ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "onnx_model_path = \"sam_onnx_example.onnx\"\n",
    "sam = sam_model_registry[model_type](checkpoint=os.path.join(checkpoint_dir, checkpoint))\n",
    "onnx_model = SamOnnxModel(sam, return_single_mask=False)\n",
    "\n",
    "dynamic_axes = {\n",
    "    \"point_coords\": {1: \"num_points\"},\n",
    "    \"point_labels\": {1: \"num_points\"},\n",
    "}\n",
    "\n",
    "embed_dim = sam.prompt_encoder.embed_dim\n",
    "embed_size = sam.prompt_encoder.image_embedding_size\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "dummy_inputs = {\n",
    "    \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
    "    \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n",
    "    \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n",
    "    \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
    "    \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
    "    \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n",
    "}\n",
    "output_names = [\"iou_predictions\", \"low_res_masks\"]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    with open(onnx_model_path, \"wb\") as f:\n",
    "        torch.onnx.export(\n",
    "            onnx_model,\n",
    "            tuple(dummy_inputs.values()),\n",
    "            f,\n",
    "            export_params=True,\n",
    "            verbose=False,\n",
    "            opset_version=17,\n",
    "            do_constant_folding=True,\n",
    "            input_names=list(dummy_inputs.keys()),\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes,\n",
    "        )\n",
    "\n",
    "# We never load the quantized model, no point in producing it\n",
    "# # If desired, the model can additionally be quantized and optimized. We find this improves web runtime significantly for negligible change in qualitative performance. Run the next cell to quantize the model, or skip to the next section otherwise.\n",
    "#\n",
    "# onnx_model_quantized_path = \"sam_onnx_quantized_example.onnx\"\n",
    "# quantize_dynamic(\n",
    "#     model_input=onnx_model_path,\n",
    "#     model_output=onnx_model_quantized_path,\n",
    "#     optimize_model=True,\n",
    "#     per_channel=False,\n",
    "#     reduce_range=False,\n",
    "#     weight_type=QuantType.QUInt8,\n",
    "# )\n",
    "# onnx_model_path = onnx_model_quantized_path\n",
    "\n",
    "wrapper_model = MyModelWrapper(onnx_model)\n",
    "trace = torch.jit.trace(wrapper_model.eval(), tuple(dummy_inputs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759bfb6-84d5-4e2f-bf5c-fbd8bb5fe4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_inputs = {\n",
    "    \"image_embeddings\": ct.TensorType(name=\"image_embeddings\", shape=dummy_inputs[\"image_embeddings\"].size()),\n",
    "    \"point_coords\": ct.TensorType(name=\"point_coords\", shape=dummy_inputs[\"point_coords\"].size()),\n",
    "    \"point_labels\": ct.TensorType(name=\"point_labels\", shape=dummy_inputs[\"point_labels\"].size()),\n",
    "    \"mask_input\": ct.TensorType(name=\"mask_input\", shape=dummy_inputs[\"mask_input\"].size()),\n",
    "    \"has_mask_input\": ct.TensorType(name=\"has_mask_input\", shape=dummy_inputs[\"has_mask_input\"].size()),\n",
    "    \"orig_im_size\": ct.TensorType(name=\"orig_im_size\", shape=dummy_inputs[\"orig_im_size\"].size()),\n",
    "}\n",
    "coreml_outputs = {\n",
    "    \"iou_predictions\": ct.TensorType(name=\"iou_predictions\"),\n",
    "    \"low_res_masks\": ct.TensorType(name=\"low_res_masks\")\n",
    "}\n",
    "model = ct.convert(\n",
    "    trace,\n",
    "    outputs=list(coreml_outputs.values()),\n",
    "    inputs=list(coreml_inputs.values()),\n",
    "    minimum_deployment_target=ct.target.iOS15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(\"sam.mlpackage\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything below this point is for exporting the entire pytorch model (embeddings and all) directly using coreml, and not the embeddings-to-masks part via the onnx model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cells allow you to test the model on a single image, without specifying prompts (points, bboxes)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from segment_anything.utils.coreml import SamEmbedder\n",
    "\n",
    "checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "# checkpoint = 'sam_vit_b_01ec64.pth'\n",
    "# model_type = 'vit_b'\n",
    "\n",
    "checkpoint_dir = '/Users/anatoli/Documents/segment-anything/checkpoints'\n",
    "\n",
    "class MyModelWrapper3(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(MyModelWrapper3, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *x):\n",
    "        output = self.model(*x)\n",
    "        # Modify output here\n",
    "        return output\n",
    "\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=os.path.join(checkpoint_dir, checkpoint))\n",
    "mymodel3 = MyModelWrapper3(SamEmbedder(sam))\n",
    "dummy_inputs_for_sam = {\n",
    "    # The image as a torch tensor in 3xHxW format, already transformed for input to the model.\n",
    "    'image': torch.randint(low=0, high=255, size=(3, 1024, 1024), dtype=torch.float),\n",
    "}\n",
    "\n",
    "sam_traced_model = torch.jit.trace(mymodel3.eval(), tuple(dummy_inputs_for_sam.values()))\n",
    "\n",
    "\n",
    "coreml_inputs = {\n",
    "    'image': ct.TensorType(name='image', shape = dummy_inputs_for_sam['image'].size()),\n",
    "}\n",
    "coreml_outputs = {\n",
    "    \"image_embeddings\": ct.TensorType(name=\"image_embeddings\")\n",
    "}\n",
    "embedder_model = ct.convert(\n",
    "    sam_traced_model,\n",
    "    outputs=list(coreml_outputs.values()),\n",
    "    inputs=list(coreml_inputs.values()),\n",
    "    minimum_deployment_target=ct.target.iOS15\n",
    ")\n",
    "\n",
    "embedder_model.save(\"sambedder.mlpackage\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "image = cv2.imread('/Users/anatoli/Downloads/Untitled_anatoli.jpeg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, mask in enumerate(masks):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    show_mask(mask['segmentation'], plt.gca())\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
