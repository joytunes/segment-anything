{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2331a21f-fa89-427b-a581-a4952d102d88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.2.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torchvision\n",
    "import coremltools as ct\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import QuantType\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76fc53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "checkpoint_dir = '.' #'/Users/anatoli/Documents/segment-anything/checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11a84e-8c1c-4d8e-b14e-f92e7ec6d02a",
   "metadata": {},
   "source": [
    "The script `segment-anything/scripts/export_onnx_model.py` can be used to export the necessary portion of SAM. Alternatively, run the following code to export an ONNX model. If you have already exported a model, set the path below and skip to the next section. Assure that the exported ONNX model aligns with the checkpoint and model type set above. This notebook expects the model was exported with the parameter `return_single_mask=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "252cea7b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MyModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(MyModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *x):\n",
    "        output = self.model(*x)\n",
    "        # Modify output here\n",
    "        return (output[1], output[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da638ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/modeling/mask_decoder.py:117: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  i = torch.as_tensor(torch.arange(x.size()[0] * n, device=x.device) // n, dtype=torch.int32)\n",
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/modeling/transformer.py:232: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  attn = attn / math.sqrt(c_per_head)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "onnx_model_path = \"sam_onnx_example.onnx\"\n",
    "sam = sam_model_registry[model_type](checkpoint=os.path.join(checkpoint_dir, checkpoint))\n",
    "onnx_model = SamOnnxModel(sam, return_single_mask=False)\n",
    "\n",
    "dynamic_axes = {\n",
    "    \"point_coords\": {1: \"num_points\"},\n",
    "    \"point_labels\": {1: \"num_points\"},\n",
    "}\n",
    "\n",
    "embed_dim = sam.prompt_encoder.embed_dim\n",
    "embed_size = sam.prompt_encoder.image_embedding_size\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "dummy_inputs = {\n",
    "    \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
    "    \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n",
    "    \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n",
    "    \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
    "    \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
    "    \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n",
    "}\n",
    "output_names = [\"iou_predictions\", \"low_res_masks\"]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    with open(onnx_model_path, \"wb\") as f:\n",
    "        torch.onnx.export(\n",
    "            onnx_model,\n",
    "            tuple(dummy_inputs.values()),\n",
    "            f,\n",
    "            export_params=True,\n",
    "            verbose=False,\n",
    "            opset_version=17,\n",
    "            do_constant_folding=True,\n",
    "            input_names=list(dummy_inputs.keys()),\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes,\n",
    "        )\n",
    "\n",
    "# We never load the quantized model, no point in producing it\n",
    "# # If desired, the model can additionally be quantized and optimized. We find this improves web runtime significantly for negligible change in qualitative performance. Run the next cell to quantize the model, or skip to the next section otherwise.\n",
    "#\n",
    "# onnx_model_quantized_path = \"sam_onnx_quantized_example.onnx\"\n",
    "# quantize_dynamic(\n",
    "#     model_input=onnx_model_path,\n",
    "#     model_output=onnx_model_quantized_path,\n",
    "#     optimize_model=True,\n",
    "#     per_channel=False,\n",
    "#     reduce_range=False,\n",
    "#     weight_type=QuantType.QUInt8,\n",
    "# )\n",
    "# onnx_model_path = onnx_model_quantized_path\n",
    "\n",
    "wrapper_model = MyModelWrapper(onnx_model)\n",
    "trace = torch.jit.trace(wrapper_model.eval(), tuple(dummy_inputs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759bfb6-84d5-4e2f-bf5c-fbd8bb5fe4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_inputs = {\n",
    "    \"image_embeddings\": ct.TensorType(name=\"image_embeddings\", shape=dummy_inputs[\"image_embeddings\"].size()),\n",
    "    \"point_coords\": ct.TensorType(name=\"point_coords\", shape=dummy_inputs[\"point_coords\"].size()),\n",
    "    \"point_labels\": ct.TensorType(name=\"point_labels\", shape=dummy_inputs[\"point_labels\"].size()),\n",
    "    \"mask_input\": ct.TensorType(name=\"mask_input\", shape=dummy_inputs[\"mask_input\"].size()),\n",
    "    \"has_mask_input\": ct.TensorType(name=\"has_mask_input\", shape=dummy_inputs[\"has_mask_input\"].size()),\n",
    "    \"orig_im_size\": ct.TensorType(name=\"orig_im_size\", shape=dummy_inputs[\"orig_im_size\"].size()),\n",
    "}\n",
    "coreml_outputs = {\n",
    "    \"iou_predictions\": ct.TensorType(name=\"iou_predictions\"),\n",
    "    \"low_res_masks\": ct.TensorType(name=\"low_res_masks\")\n",
    "}\n",
    "model = ct.convert(\n",
    "    trace,\n",
    "    outputs=list(coreml_outputs.values()),\n",
    "    inputs=list(coreml_inputs.values()),\n",
    "    minimum_deployment_target=ct.target.iOS15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e14642",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"sam.mlpackage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e7581",
   "metadata": {},
   "source": [
    "Everything below this point is for exporting the entire pytorch model (embeddings and all) directly using coreml, and not the embeddings-to-masks part via the onnx model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a34c0",
   "metadata": {},
   "source": [
    "The following cells allow you to test the model on a single image, without specifying prompts (points, bboxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfdbe34f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024, 1024])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_inputs_for_sam = {\n",
    "    # The image as a torch tensor in 3xHxW format, already transformed for input to the model.\n",
    "    'image': torch.randint(low=0, high=255, size=(3, 1024, 1024), dtype=torch.float),\n",
    "}\n",
    "\n",
    "dummy_inputs_for_sam['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35ef763",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/utils/coreml.py:52: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and input_image_torch.shape[1] == 3\n",
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/utils/coreml.py:53: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and max(*input_image_torch.shape[2:]) == self.not_model.image_encoder.img_size\n",
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/modeling/image_encoder.py:258: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_h > 0 or pad_w > 0:\n",
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/modeling/image_encoder.py:304: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/modeling/image_encoder.py:304: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/modeling/image_encoder.py:306: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if rel_pos.shape[0] != max_rel_dist:\n",
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/modeling/image_encoder.py:318: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/modeling/image_encoder.py:319: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/modeling/image_encoder.py:320: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
      "/Users/kalugny/Code/segment-anything/notebooks/../segment_anything/modeling/image_encoder.py:287: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if Hp > H or Wp > W:\n",
      "Converting PyTorch Frontend ==> MIL Ops:   0%|                                                                                                                                     | 0/1875 [00:00<?, ? ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  17%|████████████████████▊                                                                                                    | 323/1875 [00:00<00:00, 3144.13 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  38%|██████████████████████████████████████████████▏                                                                          | 715/1875 [00:00<00:00, 3590.24 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  58%|█████████████████████████████████████████████████████████████████████▍                                                  | 1084/1875 [00:00<00:00, 3633.62 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  77%|████████████████████████████████████████████████████████████████████████████████████████████▋                           | 1448/1875 [00:00<00:00, 1558.54 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎     | 1787/1875 [00:00<00:00, 1912.72 ops/s]Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Saving value type of int64 into a builtin type of int32, might overflow or loses precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1874/1875 [00:00<00:00, 2163.61 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 19.36 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [00:06<00:00,  8.94 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 521.89 passes/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "BlobWriter not loaded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     31\u001b[0m coreml_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: ct\u001b[38;5;241m.\u001b[39mImageType(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, shape \u001b[38;5;241m=\u001b[39m dummy_inputs_for_sam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize()),\n\u001b[1;32m     33\u001b[0m }\n\u001b[1;32m     34\u001b[0m coreml_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: ct\u001b[38;5;241m.\u001b[39mTensorType(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m }\n\u001b[0;32m---> 37\u001b[0m embedder_model \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43msam_traced_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcoreml_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcoreml_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimum_deployment_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miOS15\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m embedder_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msambedder.mlpackage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sam-LESymAQG-py3.11/lib/python3.11/site-packages/coremltools/converters/_converters_entry.py:492\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m specification_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     specification_version \u001b[38;5;241m=\u001b[39m _set_default_specification_version(exact_target)\n\u001b[0;32m--> 492\u001b[0m mlmodel \u001b[38;5;241m=\u001b[39m \u001b[43mmil_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs_as_tensor_or_image_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# None or list[ct.ImageType/ct.TensorType]\u001b[39;49;00m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_model_load\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_model_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackage_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecification_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecification_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmain_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exact_target \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmilinternal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mlmodel  \u001b[38;5;66;03m# Returns the MIL program\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sam-LESymAQG-py3.11/lib/python3.11/site-packages/coremltools/converters/mil/converter.py:188\u001b[0m, in \u001b[0;36mmil_convert\u001b[0;34m(model, convert_from, convert_to, compute_units, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;129m@_profile\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmil_convert\u001b[39m(\n\u001b[1;32m    151\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    156\u001b[0m ):\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Convert model from a specified frontend `convert_from` to a specified\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    converter backend `convert_to`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m        See `coremltools.converters.convert`\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mil_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConverterRegistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMLModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sam-LESymAQG-py3.11/lib/python3.11/site-packages/coremltools/converters/mil/converter.py:212\u001b[0m, in \u001b[0;36m_mil_convert\u001b[0;34m(model, convert_from, convert_to, registry, modelClass, compute_units, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     weights_dir \u001b[38;5;241m=\u001b[39m _tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory()\n\u001b[1;32m    210\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m weights_dir\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m--> 212\u001b[0m proto, mil_program \u001b[38;5;241m=\u001b[39m \u001b[43mmil_convert_to_proto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m _reset_conversion_state()\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmilinternal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sam-LESymAQG-py3.11/lib/python3.11/site-packages/coremltools/converters/mil/converter.py:303\u001b[0m, in \u001b[0;36mmil_convert_to_proto\u001b[0;34m(model, convert_from, convert_to, converter_registry, main_pipeline, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackend converter \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconvert_to\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not implemented, must be \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(converter_registry\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m     )\n\u001b[1;32m    302\u001b[0m backend_converter \u001b[38;5;241m=\u001b[39m backend_converter_type()\n\u001b[0;32m--> 303\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_converter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, prog\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sam-LESymAQG-py3.11/lib/python3.11/site-packages/coremltools/converters/mil/converter.py:130\u001b[0m, in \u001b[0;36mMILProtoBackend.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load \u001b[38;5;28;01mas\u001b[39;00m backend_load\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sam-LESymAQG-py3.11/lib/python3.11/site-packages/coremltools/converters/mil/backend/mil/load.py:283\u001b[0m, in \u001b[0;36mload\u001b[0;34m(prog, weights_dir, resume_on_errors, specification_version, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(prog, weights_dir, resume_on_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, specification_version\u001b[38;5;241m=\u001b[39m_SPECIFICATION_VERSION_IOS_15, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m BlobWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlobWriter not loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m prog\u001b[38;5;241m.\u001b[39mfunctions:\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain function not found in program\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: BlobWriter not loaded"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from segment_anything.utils.coreml import SamEmbedder\n",
    "\n",
    "# checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "# model_type = \"vit_h\"\n",
    "checkpoint = 'sam_vit_b_01ec64.pth'\n",
    "model_type = 'vit_b'\n",
    "\n",
    "checkpoint_dir = '.' #'/Users/anatoli/Documents/segment-anything/checkpoints'\n",
    "\n",
    "class MyModelWrapper3(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(MyModelWrapper3, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *x):\n",
    "        output = self.model(*x)\n",
    "        # Modify output here\n",
    "        return output\n",
    "\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=os.path.join(checkpoint_dir, checkpoint))\n",
    "mymodel3 = MyModelWrapper3(SamEmbedder(sam))\n",
    "dummy_inputs_for_sam = {\n",
    "    # The image as a torch tensor in 3xHxW format, already transformed for input to the model.\n",
    "    'image': torch.randint(low=0, high=255, size=(3, 1024, 1024), dtype=torch.float),\n",
    "}\n",
    "\n",
    "sam_traced_model = torch.jit.trace(mymodel3.eval(), tuple(dummy_inputs_for_sam.values()))\n",
    "\n",
    "\n",
    "coreml_inputs = {\n",
    "    'image': ct.ImageType(name='image', shape=dummy_inputs_for_sam['image'].shape, channel_first=True),\n",
    "}\n",
    "coreml_outputs = {\n",
    "    \"image_embeddings\": ct.TensorType(name=\"image_embeddings\")\n",
    "}\n",
    "embedder_model = ct.convert(\n",
    "    sam_traced_model,\n",
    "    outputs=list(coreml_outputs.values()),\n",
    "    inputs=list(coreml_inputs.values()),\n",
    "    minimum_deployment_target=ct.target.iOS15\n",
    ")\n",
    "\n",
    "embedder_model.save(\"sambedder.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed541c8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323943f8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "image = cv2.imread('/Users/anatoli/Downloads/Untitled_anatoli.jpeg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a6d08",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc2b02",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i, mask in enumerate(masks):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    show_mask(mask['segmentation'], plt.gca())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243dfa8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
